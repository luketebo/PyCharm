# LM (Language Model)

## 定义

通过概率分布来计算文本质量

+ 从算法的角度来讲，语言模型是对一个序列的概念分布进行建模，从而找出概率较高的序列
+ 从实际运用来讲，我们希望计算出由相同的单词不同顺序组成的序列里面，最符合人类语言表达方式的序列作为输出

## 统计语言模型

## N元 （N-gram） 语言模型

通过引入马尔可夫假设，可以降低模型的复杂度，
马尔可夫假设：指一个单词出现的概率仅与它前面出现的有限的一个或者几个单词有关，也就是说，某一个词语出现的概率只由
其前面的N-1个词语所决定，成为N元语言模型

N = 1 时，我们称为一元语言模型，每个单词出现的概率只和它自己有关
N > 1 时，

//    P(S) = P(我) * P(要去|我) * P(吃|要去) * P(冰淇淋|吃) warning

## 最大似然数法

在已知一些观测数据的情况下，根据这些数据来选择最符合观测数据的模型参数值，使得这些数据出现的概率最大

## OOV （out-of-vocabulary）

词型还原
    running => run
同义词替换
    climbing => mountain
字符级别的建模
    N-Gram
基于上下文的方法

外部知识库
    WordNet Wiki

Smoothing (平滑)

Laplace Smoothing: 强制让所有N-gram至少出现一次，只需要在分子和分母上分别做加法

$$


$$

内插法 (Interpolation)
回溯法 (BackOff)

N-Gram 分析

优点：

+ 采用极大似然估计，参数易训练
+ 完全包含了前n - 1个词的全部信息
+ 可解释性强，直观易理解

缺点：

+ 计算量大
+ 数据系数，难免出现OOV的问题
+ 缺乏长期依赖，只能建模到前n-1个词
+ 单纯的基于统计频次，泛化能力差 模型从根本上不能去分析语句的含义

## N-gram 存在典型问题

+ 泛化能力差
+ 参数量巨大
+ 无法表征词语之间的相似关系

## 统计语言模型的运用

## Bengio 提出的神经网络语言模型

由：输入层，嵌入层，隐藏层，输出层
神经网络语言模型先给每个词的连续空间中赋予一个向量（词向量），再通过神经网络去学习这种分布式表征

## 输入层

## 嵌入层分析

词向量的初始值通常是随即初始化的，在神经网络语言模型中，每个词都会被赋予一个固定长度的向量表示，改向量被称为
词向量或嵌入向量
通常情况下，词向量的初始值是通过随即抽样的方式来获得的，此外，还可以使用与训练的词向量，来初始化神经昂落语言模型中的词向量
